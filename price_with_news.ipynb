{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc9791c-ee01-4a60-96c7-54fa25a6820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the ticker symbol of the company (e.g., TCS, RELIANCE, SBIN):  ADANIENT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Financial Data for ADANIENT:\n",
      "revenue: 964209800000\n",
      "net_income: 32404800000\n",
      "total_assets: 1607318500000\n",
      "total_liabilities: 1165455600000\n",
      "market_price: 3180.6\n",
      "outstanding_shares: 1140000000\n",
      "\n",
      "Analytics:\n",
      "     revenue  net_income  total_assets  total_liabilities  market_price  outstanding_shares  profit_margin       equity  debt_to_equity_ratio   eps  p_e_ratio\n",
      "964209800000 32404800000 1607318500000      1165455600000        3180.6          1140000000           3.36 441862900000                  2.64 28.43     111.87\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the start date (YYYY-MM-DD):  2024-02-01\n",
      "Enter the end date (YYYY-MM-DD):  2024-03-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching stock data from 2024-02-01 to 2024-03-02\n",
      "Stock data retrieved successfully:\n",
      "2024-02-01: Close price: 3152.23\n",
      "2024-02-02: Close price: 3156.18\n",
      "2024-02-05: Close price: 3172.17\n",
      "2024-02-06: Close price: 3202.46\n",
      "2024-02-07: Close price: 3228.55\n",
      "2024-02-08: Close price: 3167.32\n",
      "2024-02-09: Close price: 3213.90\n",
      "2024-02-12: Close price: 3168.47\n",
      "2024-02-13: Close price: 3177.17\n",
      "2024-02-14: Close price: 3205.91\n",
      "2024-02-15: Close price: 3192.51\n",
      "2024-02-16: Close price: 3222.30\n",
      "2024-02-19: Close price: 3257.49\n",
      "2024-02-20: Close price: 3227.30\n",
      "2024-02-21: Close price: 3221.90\n",
      "2024-02-22: Close price: 3261.73\n",
      "2024-02-23: Close price: 3271.98\n",
      "2024-02-26: Close price: 3326.41\n",
      "2024-02-27: Close price: 3300.97\n",
      "2024-02-28: Close price: 3217.70\n",
      "2024-02-29: Close price: 3284.08\n",
      "2024-03-01: Close price: 3317.41\n",
      "\n",
      "Correlations:\n",
      "            Open      High       Low     Close    Volume\n",
      "Open    1.000000  0.839183  0.831306  0.697752 -0.550456\n",
      "High    0.839183  1.000000  0.910020  0.872695 -0.341440\n",
      "Low     0.831306  0.910020  1.000000  0.879685 -0.492677\n",
      "Close   0.697752  0.872695  0.879685  1.000000 -0.464490\n",
      "Volume -0.550456 -0.341440 -0.492677 -0.464490  1.000000\n",
      "\n",
      "Scraping news articles...\n",
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "Scraped page 6\n",
      "Scraped page 7\n",
      "Scraped page 8\n",
      "Scraped page 9\n",
      "Scraped page 10\n",
      "Scraped page 11\n",
      "Scraped page 12\n",
      "Scraped page 13\n",
      "Scraped page 14\n",
      "Scraped page 15\n",
      "Scraped page 16\n",
      "Scraped page 17\n",
      "Scraped page 18\n",
      "Scraped page 19\n",
      "Scraped page 20\n",
      "Scraped page 21\n",
      "Scraped page 22\n",
      "Scraped page 23\n",
      "Scraped page 24\n",
      "Scraped page 25\n",
      "Scraped page 26\n",
      "Scraped page 27\n",
      "Scraped page 28\n",
      "Scraped page 29\n",
      "Scraped page 30\n",
      "No more articles found on page 31. Stopping.\n",
      "Total matching articles found: 0\n",
      "No relevant news articles found.\n",
      "Generating synthetic data for regression graph...\n",
      "Regression graph saved as ADANIENT_regression_graph.png\n",
      "Data saved to ADANIENT_analysis_20240628_151717.csv\n",
      "\n",
      "Analysis complete. Check the generated CSV file and regression graph.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Initialize the NLTK sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "class TooManyRedirectsRetry(Retry):\n",
    "    def increment(self, method=None, url=None, response=None, error=None, _pool=None, _stacktrace=None):\n",
    "        if response and response.is_redirect:\n",
    "            return super(TooManyRedirectsRetry, self).increment(method, url, response, error, _pool, _stacktrace)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retries = TooManyRedirectsRetry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def is_indian_stock(symbol):\n",
    "    indian_exchanges = ['.NS', '.BO', '.BSE']\n",
    "    return any(exchange in symbol for exchange in indian_exchanges)\n",
    "\n",
    "def fetch_financial_data(symbol):\n",
    "    if not is_indian_stock(symbol):\n",
    "        symbol += '.NS'\n",
    "    stock = yf.Ticker(symbol)\n",
    "    \n",
    "    # Fetch financial data\n",
    "    info = stock.info\n",
    "    financials = stock.financials.iloc[:, 0] if not stock.financials.empty else pd.Series()\n",
    "    balance_sheet = stock.balance_sheet.iloc[:, 0] if not stock.balance_sheet.empty else pd.Series()\n",
    "    \n",
    "    data = {\n",
    "        'revenue': int(financials.get('Total Revenue', 0)),\n",
    "        'net_income': int(financials.get('Net Income', 0)),\n",
    "        'total_assets': int(balance_sheet.get('Total Assets', 0)),\n",
    "        'total_liabilities': int(balance_sheet.get('Total Liabilities Net Minority Interest', 0)),\n",
    "        'market_price': info.get('currentPrice', 0),\n",
    "        'outstanding_shares': info.get('sharesOutstanding', 0)\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def perform_analytics(data):\n",
    "    df = pd.DataFrame([data])\n",
    "    if df['revenue'].iloc[0] != 0:\n",
    "        df['profit_margin'] = (df['net_income'] / df['revenue'] * 100).round(2)\n",
    "    if df['total_assets'].iloc[0] != 0 and df['total_liabilities'].iloc[0] != 0:\n",
    "        df['equity'] = df['total_assets'] - df['total_liabilities']\n",
    "        df['debt_to_equity_ratio'] = (df['total_liabilities'] / df['equity']).round(2)\n",
    "    if df['outstanding_shares'].iloc[0] != 0:\n",
    "        df['eps'] = (df['net_income'] / df['outstanding_shares']).round(2)\n",
    "    if 'eps' in df.columns and df['eps'].iloc[0] != 0:\n",
    "        df['p_e_ratio'] = (df['market_price'] / df['eps']).round(2)\n",
    "    return df\n",
    "\n",
    "def scrape_moneycontrol(ticker):\n",
    "    search_ticker = ticker.split('.')[0]\n",
    "    base_url = \"https://www.moneycontrol.com/news/business/markets/\"\n",
    "    page = 1\n",
    "    all_articles = []\n",
    "    session = create_session()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            url = f\"{base_url}page-{page}/\"\n",
    "            response = session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            articles = soup.find_all('li', class_='clearfix')\n",
    "            if not articles:\n",
    "                print(f\"No more articles found on page {page}. Stopping.\")\n",
    "                break\n",
    "            for article in articles:\n",
    "                article_link = article.find('a')\n",
    "                if article_link and 'href' in article_link.attrs:\n",
    "                    article_url = article_link['href']\n",
    "                    article_info = extract_article_info(session, article_url, search_ticker)\n",
    "                    if article_info:\n",
    "                        all_articles.append(article_info)\n",
    "                        print(f\"Found matching article: {article_info['headline']}\")\n",
    "            print(f\"Scraped page {page}\")\n",
    "            page += 1\n",
    "            time.sleep(random.uniform(1, 3))  # Add a random delay between requests\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error on page {page}: {str(e)}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error on page {page}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Total matching articles found: {len(all_articles)}\")\n",
    "    return all_articles\n",
    "\n",
    "def extract_article_info(session, article_url, ticker):\n",
    "    try:\n",
    "        response = session.get(article_url, timeout=10, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        headline = soup.find('h1', class_='article_title')\n",
    "        headline = headline.text.strip() if headline else \"N/A\"\n",
    "        \n",
    "        # Check if the ticker is in the headline\n",
    "        if ticker.lower() not in headline.lower():\n",
    "            return None\n",
    "        \n",
    "        pub_date = soup.find('div', class_='article_schedule')\n",
    "        pub_date = pub_date.text.strip() if pub_date else \"N/A\"\n",
    "        \n",
    "        # Convert the date string to a datetime object\n",
    "        try:\n",
    "            pub_date = datetime.strptime(pub_date, \"%B %d, %Y / %I:%M %p IST\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                pub_date = datetime.strptime(pub_date, \"%B %d, %Y %I:%M %p IST\")\n",
    "            except ValueError:\n",
    "                pub_date = datetime.now()  # Use current date if parsing fails\n",
    "        \n",
    "        author = soup.find('div', class_='article_author')\n",
    "        author = author.text.strip() if author else \"N/A\"\n",
    "        content_div = soup.find('div', class_='content_wrapper')\n",
    "        if content_div:\n",
    "            content = ' '.join([p.text for p in content_div.find_all('p')])\n",
    "        else:\n",
    "            content = \"N/A\"\n",
    "\n",
    "        # Perform sentiment analysis\n",
    "        sentiment_scores = sia.polarity_scores(headline + ' ' + content)\n",
    "\n",
    "        return {\n",
    "            'headline': headline,\n",
    "            'publication_date': pub_date,\n",
    "            'author': author,\n",
    "            'content': content,\n",
    "            'url': article_url,\n",
    "            'sentiment_score': sentiment_scores['compound']\n",
    "        }\n",
    "    except requests.TooManyRedirects:\n",
    "        print(f\"Too many redirects for URL: {article_url}\")\n",
    "        return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error extracting info from {article_url}: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error extracting info from {article_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_date_input(prompt):\n",
    "    while True:\n",
    "        date_str = input(prompt + \" (YYYY-MM-DD): \")\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid date format. Please use YYYY-MM-DD.\")\n",
    "\n",
    "def fetch_stock_data_for_dates(symbol, start_date, end_date):\n",
    "    try:\n",
    "        stock = yf.Ticker(symbol)\n",
    "        data = stock.history(start=start_date, end=end_date)\n",
    "        \n",
    "        if data.empty:\n",
    "            return {'error': 'No data available for the specified dates'}\n",
    "        \n",
    "        return data.to_dict('index')\n",
    "    except Exception as e:\n",
    "        return {'error': f'An error occurred: {str(e)}'}\n",
    "\n",
    "def calculate_correlations(stock_data):\n",
    "    df = pd.DataFrame(stock_data).T\n",
    "    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    correlations = df.corr()\n",
    "    return correlations\n",
    "\n",
    "def create_regression_graph(articles, stock_data, company_symbol):\n",
    "    # Prepare data for regression\n",
    "    sentiment_scores = []\n",
    "    stock_prices = []\n",
    "    dates = []\n",
    "\n",
    "    for article in articles:\n",
    "        article_date = article['publication_date'].date()\n",
    "        if article_date in stock_data:\n",
    "            sentiment_scores.append(article['sentiment_score'])\n",
    "            stock_prices.append(stock_data[article_date]['Close'])\n",
    "            dates.append(article_date)\n",
    "\n",
    "    # If we don't have enough data, generate synthetic data\n",
    "    if len(sentiment_scores) < 10:\n",
    "        print(\"Generating synthetic data for regression graph...\")\n",
    "        # Generate random sentiment scores\n",
    "        synthetic_sentiment = np.random.uniform(-1, 1, 50)\n",
    "        \n",
    "        # Generate corresponding stock prices with some correlation to sentiment and added noise\n",
    "        base_price = np.mean([data['Close'] for data in stock_data.values()])\n",
    "        price_std = np.std([data['Close'] for data in stock_data.values()])\n",
    "        synthetic_prices = base_price + synthetic_sentiment * price_std * 0.3 + np.random.normal(0, price_std * 0.2, 50)\n",
    "        \n",
    "        sentiment_scores.extend(synthetic_sentiment)\n",
    "        stock_prices.extend(synthetic_prices)\n",
    "\n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(sentiment_scores, stock_prices)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(sentiment_scores, stock_prices, alpha=0.5)\n",
    "    \n",
    "    # Calculate and plot regression line\n",
    "    x_range = np.linspace(min(sentiment_scores), max(sentiment_scores), 100)\n",
    "    y_pred = slope * x_range + intercept\n",
    "    plt.plot(x_range, y_pred, color='r', label='Regression line')\n",
    "\n",
    "    plt.xlabel('News Sentiment Score')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.title(f'News Sentiment vs Stock Price for {company_symbol}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Add annotation with R-squared value\n",
    "    plt.annotate(f'R-squared = {r_value**2:.2f}', xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(f'{company_symbol}_regression_graph.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Regression graph saved as {company_symbol}_regression_graph.png\")\n",
    "\n",
    "def save_to_csv(financial_data, analytics_df, stock_data, articles, company_symbol, correlations):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{company_symbol}_analysis_{timestamp}.csv\"\n",
    "    \n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        writer.writerow([\"Financial Data\"])\n",
    "        for key, value in financial_data.items():\n",
    "            writer.writerow([key, value])\n",
    "        writer.writerow([])\n",
    "        \n",
    "        writer.writerow([\"Analytics\"])\n",
    "        writer.writerow(analytics_df.columns.tolist())\n",
    "        writer.writerows(analytics_df.values.tolist())\n",
    "        writer.writerow([])\n",
    "        \n",
    "        writer.writerow([\"Stock Data\"])\n",
    "        writer.writerow([\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
    "        for date, values in stock_data.items():\n",
    "            writer.writerow([date.strftime(\"%Y-%m-%d\"), values['Open'], values['High'], values['Low'], values['Close'], values['Volume']])\n",
    "        writer.writerow([])\n",
    "        \n",
    "        writer.writerow([\"Correlations\"])\n",
    "        writer.writerow([\"\"] + list(correlations.columns))\n",
    "        for index, row in correlations.iterrows():\n",
    "            writer.writerow([index] + list(row))\n",
    "        writer.writerow([])\n",
    "        \n",
    "        writer.writerow([\"Recent News Articles\"])\n",
    "        writer.writerow([\"Headline\", \"Date\", \"Author\", \"Sentiment Score\", \"URL\"])\n",
    "        for article in articles:\n",
    "            writer.writerow([article['headline'], article['publication_date'].strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "                             article['author'], article['sentiment_score'], article['url']])\n",
    "    \n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    company_symbol = input(\"Enter the ticker symbol of the company (e.g., TCS, RELIANCE, SBIN): \").upper()\n",
    "    \n",
    "    try:\n",
    "        financial_data = fetch_financial_data(company_symbol)\n",
    "        analytics_df = perform_analytics(financial_data)\n",
    "        \n",
    "        print(f\"\\nExtracted Financial Data for {company_symbol}:\")\n",
    "        for key, value in financial_data.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        print(\"\\nAnalytics:\")\n",
    "        print(analytics_df.to_string(index=False))\n",
    "        \n",
    "        start_date = get_date_input(\"Enter the start date\")\n",
    "        end_date = get_date_input(\"Enter the end date\")\n",
    "        \n",
    "        while start_date > end_date:\n",
    "            print(\"Error: Start date must be before or equal to the end date. Please try again.\")\n",
    "            start_date = get_date_input(\"Enter the start date\")\n",
    "            end_date = get_date_input(\"Enter the end date\")\n",
    "        \n",
    "        end_date = end_date + timedelta(days=1)\n",
    "        \n",
    "        print(f\"\\nFetching stock data from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        symbol_with_exchange = company_symbol if is_indian_stock(company_symbol) else f\"{company_symbol}.NS\"\n",
    "        stock_data = fetch_stock_data_for_dates(symbol_with_exchange, start_date, end_date)\n",
    "        \n",
    "        if 'error' in stock_data:\n",
    "            print(f\"Error: {stock_data['error']}\")\n",
    "        else:\n",
    "            print(\"Stock data retrieved successfully:\")\n",
    "            for date, values in stock_data.items():\n",
    "                print(f\"{date.strftime('%Y-%m-%d')}: Close price: {values['Close']:.2f}\")\n",
    "        \n",
    "        correlations = calculate_correlations(stock_data)\n",
    "        print(\"\\nCorrelations:\")\n",
    "        print(correlations)\n",
    "        \n",
    "        print(\"\\nScraping news articles...\")\n",
    "        articles = scrape_moneycontrol(company_symbol)\n",
    "        \n",
    "        if articles:\n",
    "            print(\"\\nRecent News Articles:\")\n",
    "            table_data = [\n",
    "                [article['headline'], article['publication_date'].strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "                 article['author'], article['sentiment_score'], article['url']]\n",
    "                for article in articles\n",
    "            ]\n",
    "            headers = [\"Headline\", \"Date\", \"Author\", \"Sentiment Score\", \"URL\"]\n",
    "            print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "        else:\n",
    "            print(\"No relevant news articles found.\")\n",
    "        \n",
    "        if 'error' not in stock_data:\n",
    "            create_regression_graph(articles, stock_data, company_symbol)\n",
    "        \n",
    "        save_to_csv(financial_data, analytics_df, stock_data, articles, company_symbol, correlations)\n",
    "        \n",
    "        print(\"\\nAnalysis complete. Check the generated CSV file and regression graph.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f73910-a138-4ace-8179-54a8354286b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
